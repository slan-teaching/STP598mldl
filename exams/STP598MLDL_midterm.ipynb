{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STP598 Machine Learning \\& Deep Learning\n",
    "## Midterm Exam (Take-home)\n",
    "### Due 11:59pm Friday Oct. 25, 2024 on Canvas\n",
    "\n",
    "### name,  id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4860381156b50ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "* In multiple linear regression, we have residual vector defined as ${\\bf e}={\\bf y} - \\hat{\\bf y} = {\\bf y} - {\\bf X} \\hat{\\boldsymbol{\\beta}}$. Prove that it is perpendicular to the column space of ${\\bf X}$, i.e. ${\\bf X}^T{\\bf e}={\\bf 0}$.\n",
    "\n",
    "* Now if we want to plot $\\hat{\\bf y}$ against ${\\bf y}$, what is the slope? Can you prove it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f9d05ef92765f1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "*Probit regression* is a binary classification model alternative to logistic regression. The link function is probit function (inverse CDF of standard normal $\\Phi^{-1}$) instead of logit function, i.e. \n",
    "\n",
    "$$\n",
    "\\Phi^{-1}(\\mathrm{Pr}(Y=1|X))=X\\beta\n",
    "$$\n",
    "\n",
    "* Write down the log-likelihood function of $\\{x_i, y_i\\}_{i=1}^n$ and answer briefly how you can find the solution $\\hat\\beta$.\n",
    "\n",
    "* Fit **digits** data (using `sklearn.datasets.load_digits` for `n_class=2`) with *logistic regression*, *probit regression*, *random forest*, and *Gaussian process classifier* respectively. Split and dataset into training and testing (e.g. $80\\%$ vs $20\\%$). Compare their testing accuracy in one table.\n",
    "Hint: probit regression is not implemented in `scikit-learn` but has been implememted in [`statsmodels`](https://www.statsmodels.org/stable/index.html). Consider [statsmodels.discrete.discrete_model.Probit](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Probit.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffbcf0badabea285",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Compare impurity measures for splitting nodes in trees.\n",
    "\n",
    "* Fill in the blanks of the table to compute Gini index, Shannon entropy and misclassification error.\n",
    "\n",
    "| | Class 1 | Class 2 | Class 3 | $\\hat p_1$ | $\\hat p_2$ | $\\hat p_3$ | Gini | Entropy | Error |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| $\\mathcal A$| 3 | 3 | 4 | | | | | | |\n",
    "| $\\mathcal A_L$| 1 | 0 | 3 | | | | | | |\n",
    "| $\\mathcal A_R$| 2 | 3 | 1 | | | | | | |\n",
    "\n",
    "* Compute the impurity reductions for the **three** measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Gaussian process is a flexible tool for modeling nonlinear functional relationship. Given data $\\{x_i, y_i\\}_{i=1}^n$, we assume the following model:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i &= f(x_i) + \\epsilon_i, \\quad \\epsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2_\\epsilon) \\\\\n",
    "f &\\sim \\mathcal{GP}(0, \\mathcal{C})\n",
    "\\end{align*}\n",
    "\n",
    "* Given a new location $x_*$, predict $\\hat y=f(x_*)$ and give the uncertainty estimate (credible interval).\n",
    "* Simulate a dataset of 1-d input $x$ and output $y$, e.g. using $y = \\sin(x) + .1*N(0,1)$, for $n_1=10$ points. Use Gaussian process to fit such dataset. Predict $x_*$ on a grid of 100 points over the defined domain ($[0,\\pi]$ for example). Now increase the data to $n_2=50$ points (may contain $n_1$ points), repeat the same prediction. Plot the following on the same graph:\n",
    "    * $n_1$ data points and $n_2$ data points with different colors (scatter plot)\n",
    "    * posterior prediction lines based on $n_1$ and $n_2$ respectively with different colors (line plot).\n",
    "    * posterior credible bands based on $n_1$ and $n_2$ respectively with different colors ([fill_between](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html))\n",
    "* Compare the plots between two cases ($n_1$ vs $n_2$). What do you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra*\n",
    "\n",
    "Please comment on this course. What suggestions do you have to improve this course?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
